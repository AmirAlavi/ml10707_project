#!/usr/bin/env python
"""Train lstm language model

Usage:
    train <input_path> [--out_path=<path> --size=<size> --activation=<act> --bidirectional --epochs=<n_epochs> --batch_size=<n_batch> --gpus=<n_gpus>]
    train (-h | --help)

Options:
    -h --help                             Show this screen.
    -o <path>, --out_path=<path>          Specify output directory where plots and models are saved. If not specified,
                                          defaults to a current date-time stamp. [default: CurTimeStamp]
    -s <size>, --size=<size>              Size of the LSTM layer. [default: 128]
    -a <act>, --activation=<act>          Activation function for LSTM layer. [default: tanh]
    -B --bidirectional                    Use a bidirectional LSTM.
    -e <n_epochs>, --epochs=<n_epochs>    Specify number of epochs to train for. [default: 100]
    -b <n_batch>, --batch_size=<n_batch>  Specify mini-batch size. [default: 512]
    -g <n_gpus>, --gpus=<n_gpus>          Specify number of gpu cards to use (Data parallelism if > 1). [default: 1]
"""
import json
import datetime
from os import makedirs
from os.path import exists, join

from docopt import docopt
import matplotlib
matplotlib.use('Agg') # For running on headless server
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding, Masking, Bidirectional, LSTM, Dense
from keras.utils import multi_gpu_model

def create_working_directory(dirname):
    if dirname == "CurTimeStamp":
        dirname = datetime.datetime.now().strftime("%Y-%m-%d_%H.%M.%S")
    if not exists(dirname):
        makedirs(dirname)
    return dirname

# def get_as_one_hot(y, num_classes):
#     return np.eye(num_classes)[y]

# TODO: hook this up to the <input_path> command line argument, and make it use actual data
def get_data(time_steps=41, mask_timestep=20):
    # Must generate samples, i.e. 41-grams.
    # Each gram is a wordID and the wordID at mask_timestep must be set to 0
    # Random data for now
    X_train = np.random.randint(low=1, high=400001, size=(10000, time_steps))
    X_valid = np.random.randint(low=1, high=400001, size=(1000, time_steps))
    X_test = np.random.randint(low=1, high=400001, size=(1000, time_steps))
    y_train = X_train[:, mask_timestep].flatten()
    y_valid = X_valid[:, mask_timestep].flatten()
    y_test = X_test[:, mask_timestep].flatten()
    X_train[:, mask_timestep] = 0
    X_valid[:, mask_timestep] = 0
    X_test[:, mask_timestep] = 0
    return X_train, y_train, X_valid, y_valid, X_test, y_test

def get_embed_weights(filename='glove.6B.50d.txt.embed_mat.npy'):
    embed_mat = np.load(filename)
    mask_embed = np.zeros(shape=(1, embed_mat.shape[1]), dtype=np.float32)
    return np.concatenate((mask_embed, embed_mat), axis=0)

def get_model(in_seq_len=41, pretrained_embed_weights=None, lstm_size=128,
              lstm_act='tanh', is_bi=False):
    if pretrained_embed_weights is not None:
        # vocab_size should be 400000 + 1 (index 0 returns all zeros, for masking)
        vocab_size = pretrained_embed_weights.shape[0]
        embed_size = pretrained_embed_weights.shape[1]
    else:
        raise ValueError("Must provide pretrained embedding weights")
    
    model = Sequential()
    model.add(
        Embedding(vocab_size, embed_size, input_length=in_seq_len,
                  mask_zero=True, weights=[pretrained_embed_weights],
                  trainable=False)
    )
    lstm = LSTM(lstm_size, activation=lstm_act)
    if is_bi:
        model.add(Bidirectional(lstm))
    else:
        model.add(lstm)
    model.add(Dense(vocab_size, activation='softmax'))
    return model

def plot_history(history, out):
    min_train_epoch = np.argmin(history.history['loss']) + 1
    min_valid_epoch = np.argmin(history.history['val_loss']) + 1
    print("Minimum train loss achieved at epoch: " + str(min_train_epoch))
    print("Minimum valid loss achieved at epoch: " + str(min_valid_epoch))
    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'valid'], loc='upper left')
    plt.savefig(out)
    plt.close()
    
    
if __name__ == "__main__":
    args = docopt(__doc__)
    n_gpus = int(args['--gpus'])
    epochs = int(args['--epochs'])
    batch = int(args['--batch_size'])
    out_path = args['--out_path']
    lstm_size = int(args['--size'])
    lstm_act = args['--activation']
    is_bi = args['--bidirectional']
    working_dir = create_working_directory(out_path)
    
    pt_embed_weights = get_embed_weights()
    print(pt_embed_weights.shape)
    if n_gpus > 1:
        print("Using multiple gpus")
        import tensorflow as tf
        with tf.device('/cpu:0'):
            template_model = get_model(in_seq_len=41, pretrained_embed_weights=pt_embed_weights, lstm_size=lstm_size, lstm_act=lstm_act, is_bi=is_bi)
        model = multi_gpu_model(template_model, n_gpus)
    else:
        template_model = get_model(in_seq_len=41, pretrained_embed_weights=pt_embed_weights, lstm_size=lstm_size, lstm_act=lstm_act, is_bi=is_bi)
        model = template_model

    template_model.summary()
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')
    X_train, y_train, X_valid, y_valid, X_test, y_test = get_data(time_steps=41, mask_timestep=20)
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch, verbose=1, validation_data=(X_valid, y_valid))
    plot_history(history, join(working_dir, "train_hist.png"))
    template_model.save(join(working_dir, "model.h5"))
    with open(join(working_dir, "command_line_args.json"), 'w') as f:
        json.dump(args, f)
