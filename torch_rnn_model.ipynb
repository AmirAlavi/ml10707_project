{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = torch.cuda.LongTensor(np.loadtxt('Xtrain.tsv', delimiter=\"\\t\", dtype=int))\n",
    "Ytrain = torch.cuda.LongTensor(np.loadtxt('Ytrain.tsv', delimiter=\"\\t\", dtype=int))\n",
    "Xvalid = torch.cuda.LongTensor(np.loadtxt('Xvalid.tsv', delimiter=\"\\t\", dtype=int))\n",
    "Yvalid = torch.cuda.LongTensor(np.loadtxt('Yvalid.tsv', delimiter=\"\\t\", dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smallXtrain = Xtrain[0:40,:]\n",
    "smallYtrain = Ytrain[0:40]\n",
    "smallXvalid = Xvalid[0:40,:]\n",
    "smallYvalid = Yvalid[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86402, 3])\n",
      "torch.Size([86402])\n"
     ]
    }
   ],
   "source": [
    "print Xtrain.size()\n",
    "print Ytrain.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3])\n",
      "torch.Size([40])\n",
      "torch.Size([86402, 3])\n",
      "torch.Size([86402])\n"
     ]
    }
   ],
   "source": [
    "print smallXtrain.size()\n",
    "print smallYtrain.size()\n",
    "print Xtrain.size()\n",
    "print Ytrain.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = TensorDataset(Xtrain, Ytrain)\n",
    "validData = TensorDataset(Xvalid, Yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computePerplexity(output, target):\n",
    "    m = output.size()[0]\n",
    "    logsoftmax = nn.LogSoftmax()\n",
    "    final_out = logsoftmax(output)\n",
    "    targetUnsqueezed = torch.unsqueeze(target, 1)\n",
    "    expon = torch.sum(torch.gather(final_out, 1, targetUnsqueezed))\n",
    "    perplexity = np.power(2, -1.0/m * expon.data[0]/np.log(2))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageHyperParameter(object):\n",
    "    def __init__(self, gramSize, embeddingDimension, numHiddenUnits, numWords, numEpochs, lr, batchSize):\n",
    "        self.gramSize = gramSize\n",
    "        self.embeddingDimension = embeddingDimension\n",
    "        self.numHiddenUnits = numHiddenUnits\n",
    "        self.numWords = numWords\n",
    "        self.numEpochs = numEpochs\n",
    "        self.lr = lr\n",
    "        self.batchSize = batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentLanguageModeler(nn.Module):\n",
    "    def __init__(self, hyperparams):\n",
    "        super(RecurrentLanguageModeler, self).__init__()\n",
    "        numWords = hyperparams.numWords\n",
    "        embeddingDimension = hyperparams.embeddingDimension\n",
    "        self.contextSize = hyperparams.gramSize-1\n",
    "        self.numHiddenUnits = hyperparams.numHiddenUnits\n",
    "        self.embeddings = nn.Embedding(numWords, embeddingDimension)\n",
    "        self.rnn = nn.RNN(input_size=embeddingDimension,\n",
    "                          hidden_size=self.numHiddenUnits,\n",
    "                          nonlinearity='tanh',\n",
    "                          batch_first=False)\n",
    "        self.h2o = nn.Linear(numHiddenUnits, numWords)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeds = self.embeddings(X)\n",
    "        embeds = embeds.permute(1,0,2)\n",
    "        h0 = Variable(torch.zeros(1\n",
    "                                  ,embeds.size()[1]\n",
    "                                  ,self.numHiddenUnits)\n",
    "                                  .type(torch.cuda.FloatTensor))\n",
    "        rnn_out, h_n = self.rnn(embeds, h0)\n",
    "        h_n = torch.squeeze(h_n)\n",
    "        output = self.h2o(h_n)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(trainData, validData, hyperparams):\n",
    "    numHiddenUnits = hyperparams.numHiddenUnits\n",
    "    batchSize = hyperparams.batchSize\n",
    "    contextSize = hyperparams.gramSize-1\n",
    "    numEpochs = hyperparams.numEpochs\n",
    "    m = len(trainData)\n",
    "    n = len(validData)\n",
    "    model = RecurrentLanguageModeler(hyperparams)\n",
    "    model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=hyperparams.lr, momentum=0.0)\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    stats = []\n",
    "    for e in xrange(numEpochs):\n",
    "        total_loss = 0\n",
    "        dataloader = DataLoader(trainData, batch_size=hyperparams.batchSize, shuffle=True)\n",
    "        for b, databatch in enumerate(dataloader):\n",
    "            Xbatch, Ybatch = databatch\n",
    "            Xbatch = Variable(Xbatch)\n",
    "            Ybatch = Variable(Ybatch.type(torch.cuda.LongTensor))\n",
    "            output = model(Xbatch)\n",
    "            loss =  criterion(output, Ybatch)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "        validDataLoader = DataLoader(validData, batch_size=n, shuffle=False)\n",
    "        totalValidationLoss = 0\n",
    "        for b, databatch in enumerate(validDataLoader):\n",
    "            Xbatch, Ybatch = databatch\n",
    "            Xbatch = Variable(Xbatch)\n",
    "            Ybatch = Variable(Ybatch.type(torch.cuda.LongTensor))\n",
    "            output = model(Xbatch)\n",
    "            loss = criterion(output, Ybatch)\n",
    "            perplexity = computePerplexity(output, Ybatch)\n",
    "            totalValidationLoss += loss.data[0]\n",
    "        stats.append([e+1, total_loss/m, totalValidationLoss/n, perplexity])\n",
    "        print e+1, total_loss/m, totalValidationLoss/n, perplexity\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numWords = 8000\n",
    "embeddingDimension = 16\n",
    "numHiddenUnits = 128\n",
    "numEpochs = 100\n",
    "gramSize = 4\n",
    "lr = 0.01\n",
    "batchSize = 16\n",
    "hyperparams = LanguageHyperParameter(gramSize, embeddingDimension, numHiddenUnits, numWords, numEpochs, lr, batchSize)\n",
    "experiment_name = \"torch_rnn_language_\"+\"_h_\"+str(numHiddenUnits)+\"_lr_\"+str(lr)+\"_b_\"+str(batchSize)+\"_ed_\"+str(embeddingDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AddmmBackward' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-31f44925c399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-e2f39befc161>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(trainData, validData, hyperparams)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mprint\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AddmmBackward' object is not callable"
     ]
    }
   ],
   "source": [
    "stats = trainModel(trainData, validData, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stats = np.array(stats)\n",
    "epochs = stats[:,0]\n",
    "train_loss = stats[:,1]\n",
    "val_loss = stats[:,2]\n",
    "val_perplexity = stats[:,3]\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, 'b', epochs, val_loss, 'r')\n",
    "plt.ylim(ymin=0)    \n",
    "plt.title('Loss plot')\n",
    "plt.savefig(experiment_name+\"loss_plot.pdf\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, val_perplexity)\n",
    "plt.ylim(ymin=0)    \n",
    "plt.title('Perplexity plot')\n",
    "plt.savefig(experiment_name+\"perplexity_plot.pdf\")\n",
    "\n",
    "\n",
    "with open(experiment_name+\".language_stats\",'w') as f:\n",
    "    for stat in stats:\n",
    "        f.write(\"\\t\".join([str(val) for val in stat]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
